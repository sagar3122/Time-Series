{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Validate",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagar3122/Time-Series/blob/master/LSTM_Validate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jFesVy-hFiQG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#frame a sequence as a supervised learning problem\n",
        "\n",
        "def timeseries_to_supervised(data, lag = 1):\n",
        "    df1 = pd.DataFrame(data)\n",
        "    # print(df1)\n",
        "    columns = [df1.shift(i) for i in range(1, lag+1)]\n",
        "    #print(columns)\n",
        "    columns.append(df1)\n",
        "    # print(type(columns))\n",
        "    # print(columns)\n",
        "    df1 = pd.concat(columns, axis = 1)\n",
        "    # print(columns)\n",
        "    df1.fillna(0, inplace = True)\n",
        "    # print(df1)\n",
        "    return df1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#transform to be stationary\n",
        "#create a differenced series\n",
        "def difference(dataset, interval = 1):\n",
        "    diff = list()\n",
        "    for i in range(interval, len(dataset)):\n",
        "        # print(\"dataset.iloc[i]\", dataset.iloc[i])\n",
        "        # print(\"dataset.iloc[i-interval]\", dataset.iloc[i-interval])\n",
        "        value = dataset.iloc[i] - dataset.iloc[i-interval]\n",
        "        # print(value)\n",
        "        diff.append(value)\n",
        "    return pd.DataFrame(diff)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#invert the transform\n",
        "#invert differenced value\n",
        "def inverse_difference(original_dataset, yhat, interval = 1):\n",
        "    # print(yhat, -interval)\n",
        "    # print(\"yhat + original_dataset[-interval]\", yhat, original_dataset.iloc[-interval])\n",
        "    return yhat + original_dataset.iloc[-interval]\n",
        "\n",
        "# inverted = list()\n",
        "# for i in range(len(differenced)):\n",
        "#     value = inverse_difference(df, differenced.iloc[i], len(df)-i)\n",
        "#     inverted.append(value)\n",
        "# inverted = pd.DataFrame(inverted)\n",
        "# print(inverted.head())\n",
        "# print(inverted.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#scale train and test data to [-1,1]\n",
        "def scale(train,test):\n",
        "    #fit the scaler\n",
        "    scaler = MinMaxScaler(feature_range = (-1,1))\n",
        "    scaler = scaler.fit(train)\n",
        "    #transform train\n",
        "    # train = train.reshape(train.shape[0],train.shape[1])\n",
        "    # print(train.shape)\n",
        "    # print(train)\n",
        "    train_scaled = scaler.transform(train)\n",
        "    #transform test\n",
        "    # test = test.reshape(test.shape[0],test.shape[1])\n",
        "    # print(test.shape)\n",
        "    # print(test)\n",
        "    test_scaled = scaler.transform(test)\n",
        "    return scaler, train_scaled, test_scaled\n",
        "\n",
        "#inverse scaling for a forecasted value\n",
        "def invert_scale(scaler, X, value):\n",
        "    new_row = [x for x in X] + [value]\n",
        "    array = np.array(new_row)\n",
        "    array = array.reshape(1, len(array))\n",
        "    inverted =  scaler.inverse_transform(array)\n",
        "    return inverted[0,-1]\n",
        "\n",
        "#fit an LSTM network to training data\n",
        "def fit_LSTM(train, batch_size, nb_epoch, neurons):\n",
        "    X, y = train[:, 0:-1], train[:, -1]\n",
        "    # print(X)\n",
        "    # print(X.shape)\n",
        "    # print(y)\n",
        "    # print(y.shape)\n",
        "    X = X.reshape(X.shape[0], 1, X.shape[1]) #this is the 3D input to the LSTM network- samples, timesteps, features\n",
        "    # print(X)\n",
        "    # print(X.shape)\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(neurons, batch_input_shape = (batch_size, X.shape[1], X.shape[2]), stateful = True))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
        "    for i in range(nb_epoch):\n",
        "        print(\"epoch\", i)\n",
        "        model.fit(X,y, epochs = 1, batch_size = batch_size, verbose = 0, shuffle = False)\n",
        "        model.reset_states()\n",
        "    return model\n",
        "\n",
        "#make a one step forcast\n",
        "def forecast_LSTM(model,batch_size, X):\n",
        "    X = X.reshape(1,1,len(X))\n",
        "    yhat = model.predict(X, batch_size = batch_size)\n",
        "    return yhat[0,0]\n",
        "\n",
        "\n",
        "#load the data set\n",
        "df = pd.read_csv('Train_SU63ISt.csv', index_col = 1)\n",
        "df = df.drop(columns = ['ID'])\n",
        "df = df.astype('float64')\n",
        "print(type(df))\n",
        "print(df.head())\n",
        "print(df.shape)\n",
        "\n",
        "#transform data to stationary\n",
        "diff_values = difference(df, 1)\n",
        "print(type(diff_values))\n",
        "print(diff_values.head())\n",
        "print(diff_values.shape)\n",
        "\n",
        "#transform data to be supervised learning\n",
        "supervised = timeseries_to_supervised(diff_values,1)\n",
        "print(supervised.head())\n",
        "print(supervised.shape)\n",
        "\n",
        "#split the data into train and test sets # a rough 80-20 split\n",
        "train = supervised[0:15001]\n",
        "validate = supervised[15001:]\n",
        "print(train)\n",
        "print(train.shape)\n",
        "print(validate)\n",
        "print(validate.shape)\n",
        "\n",
        "#transform the scale of both data sets\n",
        "scaler, train_scaled, validate_scaled = scale(train, validate)\n",
        "print(train_scaled)\n",
        "print(train_scaled.shape)\n",
        "print(type(train_scaled))\n",
        "print(validate_scaled)\n",
        "print(validate_scaled.shape)\n",
        "print(type(validate_scaled))\n",
        "\n",
        "#fit the model\n",
        "LSTM_model = fit_LSTM(train_scaled, 1, 10, 1)\n",
        "#forecast the entire training data set to build up state for forecasting #seeding the intial state for the model prior forcasting for the validate/test data.\n",
        "train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)  #this is the 3D input to the LSTM network- samples, timesteps, features\n",
        "# print(train_reshaped)\n",
        "# print(train_reshaped.shape)\n",
        "LSTM_model.predict(train_reshaped, batch_size=1)\n",
        "\n",
        "# walk-forward validation on validation data set\n",
        "# predictions = list()\n",
        "# for i in range(len(validate_scaled)):\n",
        "#     #make one-step forecast\n",
        "#     X, y = validate_scaled[i, 0:-1], validate_scaled[i, -1]\n",
        "#     print(\"X\",X)\n",
        "#     print(\"y\",y)\n",
        "#     yhat = forecast_LSTM(LSTM_model, 1, X)\n",
        "#     print(\"yhat\",yhat)\n",
        "#     #doing walk forward dynamically\n",
        "#     #invert scale\n",
        "#     yhat = invert_scale(scaler, X, yhat)\n",
        "#     groundtruth = invert_scale(scaler, X, y)\n",
        "#     #invert differencing\n",
        "#     yhat = inverse_difference(df, yhat, len(validate_scaled) + 1-i)\n",
        "#     print(yhat)\n",
        "#     print(type(yhat))\n",
        "#     groundtruth = inverse_difference(df, groundtruth, len(validate_scaled) + 1-i)\n",
        "#     #store forecast\n",
        "#     predictions.append(yhat.values[0])\n",
        "#     expected = df.iloc[len(train) + i + 1]\n",
        "#     df_index = len(train) + i + 1\n",
        "#     print(\"df_index\", df_index)\n",
        "#     index_value = len(train) + i\n",
        "#     print(\"index_value\", index_value)\n",
        "#     print('Datetime=%d, Predicted=%f, Expected=%f, groundtruth=%f' % (validate.index.values[i], yhat, expected, groundtruth))\n",
        "# #\n",
        "# #\n",
        "# #\n",
        "# # report performance\n",
        "# groundtruths = df.iloc[15002:,0].values.tolist()\n",
        "# # print(\"groundtruths: \", groundtruths)\n",
        "# print(len(groundtruths))\n",
        "# # print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\npredictions\", predictions)\n",
        "# print(len(predictions))\n",
        "# rmse = sqrt(mean_squared_error(groundtruths, predictions))\n",
        "# print('Test RMSE: %.3f' % rmse)\n",
        "# # # line plot of observed vs predicted\n",
        "# # # pyplot.plot(raw_values[-12:])\n",
        "# # # pyplot.plot(predictions)\n",
        "# # # pyplot.show()\n",
        "#\n",
        "#walk-forward validation on validation data set with predictions made\n",
        "validate_auto_scaled = np.zeros((validate_scaled.shape[0],validate_scaled.shape[1]))\n",
        "# validate_auto_scaled = validate_auto_scaled.reshape(validate_scaled.shape[0],validate_scaled.shape[1])\n",
        "validate_auto_scaled[0,0] = train_scaled[-1,1]\n",
        "print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nvalidate_auto_scaled\",validate_auto_scaled)\n",
        "print(type(validate_auto_scaled))\n",
        "print(validate_auto_scaled.shape)\n",
        "\n",
        "\n",
        "predictions = list()\n",
        "for i in range(len(validate_auto_scaled)):\n",
        "    #make one-step forecast\n",
        "    X = validate_auto_scaled[i, 0:-1]\n",
        "    print(\"X\",X)\n",
        "    # print(\"y\",y)\n",
        "    yhat = forecast_LSTM(LSTM_model, 1, X)\n",
        "    # print(\"yhat\",yhat)\n",
        "    validate_auto_scaled[i, -1] = yhat\n",
        "    try:\n",
        "        validate_auto_scaled[i+1, 0] = yhat\n",
        "    except:\n",
        "        pass\n",
        "    print(\"y\"+str(i), validate_auto_scaled[i, -1])\n",
        "    try:\n",
        "        print(\"x\"+str(i+1), validate_auto_scaled[i+1, 0])\n",
        "    except:\n",
        "        pass\n",
        "    #doing walk forward dynamically\n",
        "    #invert scale\n",
        "    yhat = invert_scale(scaler, X, yhat)\n",
        "    groundtruth = invert_scale(scaler, X, validate_auto_scaled[i, -1])\n",
        "    #invert differencing\n",
        "    yhat = inverse_difference(df, yhat, len(validate_scaled) + 1-i)\n",
        "    groundtruth = inverse_difference(df, groundtruth, len(validate_scaled) + 1-i)\n",
        "    #store forecast\n",
        "    predictions.append(yhat.values[0])\n",
        "    expected = df.iloc[len(train) + i + 1]\n",
        "    df_index = len(train) + i + 1\n",
        "    index_value = len(train) + i\n",
        "    print(\"index_value\", index_value)\n",
        "    # original_index = df.index.values[len(train) + i]\n",
        "    print('Datetime=%d, Predicted=%f, Expected=%f, groundtruth=%f' % (validate.index.values[i], yhat, expected, groundtruth))\n",
        "\n",
        "\n",
        "# report performance\n",
        "groundtruths = df.iloc[15002:,0].values.tolist()\n",
        "# print(\"groundtruths: \", groundtruths)\n",
        "print(len(groundtruths))\n",
        "# print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\npredictions\", predictions)\n",
        "print(len(predictions))\n",
        "rmse = sqrt(mean_squared_error(groundtruths, predictions))\n",
        "print('Test RMSE: %.3f' % rmse)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}