{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TS_ARIMA",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagar3122/Time-Series/blob/master/TS_ARIMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-Bn9QZkAGs3w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pylab import rcParams\n",
        "from datetime import datetime\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.api import Holt, ExponentialSmoothing\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "\n",
        "train = pd.read_csv(\"Train_SU63ISt.csv\")\n",
        "test = pd.read_csv(\"Test_0qrQsBZ.csv\")\n",
        "#print(train)\n",
        "train_original = train.copy()\n",
        "test_original = test.copy()\n",
        "# print(train.columns)\n",
        "# print(test.columns)\n",
        "# print(train.dtypes)\n",
        "# print(test.dtypes)\n",
        "# print(train.shape)\n",
        "# print(test.shape)\n",
        "train[\"Datetime\"] = pd.to_datetime(train.Datetime, format = \"%d-%m-%Y %H:%M\")\n",
        "test[\"Datetime\"] = pd.to_datetime(test.Datetime, format = \"%d-%m-%Y %H:%M\")\n",
        "test_original['Datetime'] = pd.to_datetime(test_original.Datetime,format='%d-%m-%Y %H:%M')\n",
        "train_original['Datetime'] = pd.to_datetime(train_original.Datetime,format='%d-%m-%Y %H:%M')\n",
        "# print(train.dtypes)\n",
        "# print(test.dtypes)\n",
        "\n",
        "for i in [train, test, train_original, test_original]:\n",
        "    i[\"Year\"] = i.Datetime.dt.year\n",
        "    i[\"Month\"] = i.Datetime.dt.month\n",
        "    i[\"Day\"] = i.Datetime.dt.day\n",
        "    i[\"Hour\"] = i.Datetime.dt.hour\n",
        "\n",
        "#print(train.dtypes, test.dtypes)\n",
        "#print(train)\n",
        "train[\"day of week\"] = train.Datetime.dt.dayofweek # .dayofweek attribute returns the number associated with the day, based on date, month and year(just like a calender)\n",
        "train[\"day name\"] = train.Datetime.dt.day_name() #day_name() returns the name of the day, based on date, month and year(just like a calender)\n",
        "#temp = train[\"Datetime\"] # temp is a series\n",
        "#print(temp)\n",
        "#print(train)\n",
        "\n",
        "def weekend_not_weekend(row):\n",
        "    if row.day_name() == \"Saturday\" or row.day_name() == \"Sunday\": # this if block does not work because day_name() is a datetime data type i am trying to match with a string data type\n",
        "        return 1\n",
        "    # elif row.dayofweek == 5 or row.dayofweek == 6:\n",
        "    #     return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "temp2 = train[\"Datetime\"].apply(weekend_not_weekend) #temp2 is a series # .apply() is a method in pandas used to apply a some other function over a axis of the data frame, could also be applied to a series/column of a data frame.\n",
        "train[\"weekend\"] = temp2 # series gets added to data frame with a new column name weekend\n",
        "#print(temp2)\n",
        "#print(train)\n",
        "\n",
        "#train.index = train['Datetime'] # indexing the Datetime to the train df, so as to get the time period on the x-axis.\n",
        "\n",
        "df = train.drop('ID',1)           # drop ID variable to get only the Datetime on x-axis.\n",
        "#print(df)\n",
        "ts = df['Count'] # Series of passenger count with index as the datetime.\n",
        "#print(ts)\n",
        "# plt.figure(figsize=(16,8)) #create a new figure with width 16 and height 8 inches.\n",
        "# plt.plot(ts, label='Passenger Count')\n",
        "# plt.title('Time Series')\n",
        "# plt.xlabel(\"Time(year-month)\")\n",
        "# plt.ylabel(\"Passenger count\")\n",
        "# plt.legend(loc='best')\n",
        "#plt.show()\n",
        "#train.groupby(\"Year\")[\"Count\"].mean().plot(kind=\"bar\")\n",
        "# train = train.groupby(\"Year\")[\"Count\"].mean()\n",
        "# print(train)\n",
        "#train.groupby(\"Month\")[\"Count\"].mean().plot.bar()\n",
        "# temp = train.groupby([\"Year\",\"Month\"])[\"Count\"].mean()\n",
        "# temp.plot(figsize = (15,5), title = \"Passsenger Count(MonthWise)\", fontsize = \"14\")\n",
        "#train.groupby(\"Day\")[\"Count\"].mean().plot.bar()\n",
        "#train.groupby(\"Hour\")[\"Count\"].mean().plot.bar()\n",
        "#train.groupby(\"weekend\")[\"Count\"].mean().plot.bar()\n",
        "#train.groupby(\"day of week\")[\"Count\"].mean().plot.bar()\n",
        "train = train.drop(\"ID\",1)\n",
        "#print(train)\n",
        "#plt.show()\n",
        "train.Timestamp = pd.to_datetime(train.Datetime,format = \"%d-%m-%Y %H:%M\")\n",
        "#print(train)\n",
        "train.index = train.Timestamp\n",
        "hourly = train.resample(\"H\")[\"Count\"].mean()\n",
        "#print(hourly)\n",
        "daily = train.resample(\"D\")[\"Count\"].mean()\n",
        "#print(daily)\n",
        "weekly = train.resample(\"W\")[\"Count\"].mean()\n",
        "#print(weekly)\n",
        "monthly = train.resample(\"M\")[\"Count\"].mean()\n",
        "#print(monthly)\n",
        "#print(train)\n",
        "# fig, axs = plt.subplots(4,1)\n",
        "#\n",
        "# hourly.plot(figsize=(15,8), title= 'Hourly', fontsize=14, ax=axs[0])\n",
        "# daily.plot(figsize=(15,8), title= 'Daily', fontsize=14, ax=axs[1])\n",
        "# weekly.plot(figsize=(15,8), title= 'Weekly', fontsize=14, ax=axs[2])\n",
        "# monthly.plot(figsize=(15,8), title= 'Monthly', fontsize=14, ax=axs[3])\n",
        "\n",
        "#plt.show()\n",
        "# print(test)\n",
        "test.Timestamp = pd.to_datetime(test.Datetime,format='%d-%m-%Y %H:%M')\n",
        "test.index = test.Timestamp\n",
        "\n",
        "# Converting to daily mean\n",
        "#Taking the mean of all columns grouped by date\n",
        "test = test.resample('D').mean()\n",
        "# print(test)\n",
        "\n",
        "train.Timestamp = pd.to_datetime(train.Datetime,format='%d-%m-%Y %H:%M')\n",
        "train.index = train.Timestamp\n",
        "\n",
        "# Converting to daily mean\n",
        "#taking the mean of all columns grouped by date\n",
        "train = train.resample('D').mean()\n",
        "\n",
        "#splitting the data into training and validation part\n",
        "Train = train.ix[\"2012-08-25\":\"2014-06-24\"]\n",
        "valid = train.ix[\"2014-06-25\":\"2014-09-25\"]\n",
        "# print(Train)\n",
        "# print(valid)\n",
        "\n",
        "#look how the train and validation parts have been divided\n",
        "# Train.Count.plot(figsize=(15,8), title = \"Daily Ridership\", fontsize = 14, label = \"train\")\n",
        "# valid.Count.plot(figsize=(15,8), title = \"Daily Ridership\", fontsize = 14, label = \"valid\")\n",
        "# plt.xlabel(\"DateTime\")\n",
        "# plt.ylabel(\"Passenger Count\")\n",
        "# plt.legend(loc=\"best\")\n",
        "# plt.show()\n",
        "\n",
        "#visualize the original time series data, trend in the time-series data, seasonality in the time-series data and the residual, from the training data set.\n",
        "# result = sm.tsa.seasonal_decompose(Train.Count)\n",
        "# print(result)\n",
        "# sm.tsa.seasonal_decompose(Train.Count).plot()\n",
        "# plt.show()\n",
        "#adfuller test to check for unit root in the test series\n",
        "#The Augmented Dickey-Fuller test can be used to test for a unit root in a univariate process in the presence of serial correlation.\n",
        "result = sm.tsa.stattools.adfuller(train.Count)\n",
        "# print(result)\n",
        "\n",
        "\n",
        "#holt's linear trend Model\n",
        "# hltm = valid.copy()\n",
        "# fit1 = Holt(np.asarray(Train['Count'])).fit(smoothing_level = 0.3, smoothing_slope = 0.1)\n",
        "# hltm['Holt_Linear'] = fit1.forecast(len(valid))\n",
        "# # print(hltm)\n",
        "# plt.figure(figsize = (16,8))\n",
        "# plt.plot(Train['Count'], label = 'train')\n",
        "# plt.plot(valid['Count'], label = 'valid')\n",
        "# plt.plot(hltm['Holt_Linear'], label = 'holts_linear')\n",
        "# plt.legend(loc = 'best')\n",
        "# plt.show()\n",
        "\n",
        "#holt's winter model\n",
        "# hwm = valid.copy()\n",
        "# fit_1 = ExponentialSmoothing(np.asarray(Train['Count']), seasonal_periods = 7, trend = 'add', seasonal = 'add').fit()\n",
        "# hwm['Holts_winter_model'] = fit_1.forecast(len(valid))\n",
        "# plt.figure(figsize = (16,8))\n",
        "# plt.plot(Train['Count'], label = 'train')\n",
        "# plt.plot(valid['Count'], label = 'valid')\n",
        "# plt.plot(hwm['Holts_winter_model'], label = 'Holts_winter_model')\n",
        "# plt.legend(loc = 'best')\n",
        "# plt.show()\n",
        "\n",
        "# print(train_original)\n",
        "#ARIMA Model\n",
        "#Dickey Fuller test for stationarity\n",
        "def test_stationarity(timeseries):\n",
        "\n",
        "    #Determing rolling statistics\n",
        "    rolmean = timeseries.rolling(24).mean() # 24 hours on each day\n",
        "    # print(rolmean)\n",
        "    # print(type(rolmean))\n",
        "    rolstd = timeseries.rolling(24).std()\n",
        "\n",
        "    #Plot rolling statistics:\n",
        "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
        "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
        "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Rolling Mean & Standard Deviation')\n",
        "    plt.show()\n",
        "\n",
        "    #Perform Dickey-Fuller test:\n",
        "    print('Results of Dickey-Fuller Test:')\n",
        "    dftest = adfuller(timeseries, autolag='AIC')\n",
        "    # print('dftest')\n",
        "    # print(dftest)\n",
        "    # print('break')\n",
        "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
        "    for key,value in dftest[4].items():\n",
        "        dfoutput['Critical Value (%s)'%key] = value\n",
        "    print(dfoutput)\n",
        "\n",
        "rcParams['figure.figsize'] = 20,10\n",
        "test_stationarity(train_original['Count'])\n",
        "\n",
        "\n",
        "#Removing the trend from the time series\n",
        "#Apply log transformation\n",
        "# print(Train.Count)\n",
        "# print('break')\n",
        "Train_log = np.log(Train['Count'])\n",
        "# print(Train_log)\n",
        "# print('break')\n",
        "# print(np.exp(Train_log))\n",
        "valid_log = np.log(valid['Count'])\n",
        "#take rolling average to remove the trend\n",
        "moving_avg = Train_log.rolling(24).mean()\n",
        "# plt.plot(Train_log)\n",
        "# plt.plot(moving_avg, color = 'r')\n",
        "# plt.show()\n",
        "#remove the increasing trend\n",
        "# print(Train['Count'])\n",
        "# print('break')\n",
        "# print(Train_log)\n",
        "# print('break')\n",
        "# print(moving_avg)\n",
        "# print('break')\n",
        "train_log_moving_avg_diff = Train_log - moving_avg\n",
        "# print(train_log_moving_avg_diff)\n",
        "#drop the first 23 NaN values in the difference\n",
        "train_log_moving_avg_diff.dropna(inplace = True)\n",
        "# test_stationarity(train_log_moving_avg_diff)\n",
        "#differencing to make the mean stable to make the series more stable and stationary and remove the trend\n",
        "# print(Train_log)\n",
        "# print('break')\n",
        "# print(Train_log.shift(1))\n",
        "train_log_diff = Train_log - Train_log.shift(1)\n",
        "# print(train_log_diff)\n",
        "#test_stationarity(train_log_diff.dropna())\n",
        "#removing the seasonality\n",
        "#print(pd.DataFrame(Train_log).Count.values)\n",
        "#freq used in the next line overrides the periodicity of pandas object with the timeseries index\n",
        "decomposition = seasonal_decompose(pd.DataFrame(Train_log).Count.values,freq = 24)#daily based seasonal decomposition\n",
        "#used to access the 3 different components after seasonal decomposition\n",
        "trend = decomposition.trend\n",
        "# print(trend)\n",
        "# print(type(trend))\n",
        "# print(\"break\")\n",
        "seasonal = decomposition.seasonal\n",
        "# print(seasonal)\n",
        "# print(\"break\")\n",
        "residual = decomposition.resid\n",
        "# print(residual)\n",
        "# print(\"break\")\n",
        "#plotting the seasonal decompose\n",
        "# plt.subplot(411)\n",
        "# plt.plot(Train_log, label = 'Original')\n",
        "# plt.legend(loc = 'best')\n",
        "# plt.subplot(412)\n",
        "# plt.plot(trend, label = 'trend')\n",
        "# plt.legend(loc = 'best')\n",
        "# plt.subplot(413)\n",
        "# plt.plot(seasonal, label = 'seasonal')\n",
        "# plt.legend(loc = 'best')\n",
        "# plt.subplot(414)\n",
        "# plt.plot(residual, label = 'residual')\n",
        "# plt.legend(loc = 'best')\n",
        "# plt.show()\n",
        "\n",
        "#stationarity of the residuals\n",
        "# train_log_decompose = pd.DataFrame(residual)\n",
        "# #print(Train_log.index)\n",
        "# train_log_decompose['date'] = Train_log.index\n",
        "# train_log_decompose.set_index('date', inplace = True)\n",
        "# # print(train_log_decompose)\n",
        "# train_log_decompose.dropna(inplace = True)\n",
        "# print(train_log_decompose[0])# providing a series for test\n",
        "# test_stationarity(train_log_decompose[0])\n",
        "\n",
        "\n",
        "#fitting the ARIMA model on the training data and then forcasting on the test data\n",
        "# to fit ARIMA we need to find p,d,q parameters, for that we import acf and pacf form statsmodels.tsa.stattools\n",
        "lag_acf = acf(train_log_diff.dropna(), nlags = 25)\n",
        "lag_pacf = pacf(train_log_diff.dropna(), nlags = 25, method = 'ols')\n",
        "#plot acf and pacf\n",
        "# plt.plot(lag_acf)\n",
        "# #adds a horizontal line\n",
        "# plt.axhline(y=0,linestyle='--',color='gray')\n",
        "# #lines to show the range of confidence interval\n",
        "# plt.axhline(y=-1.96/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray')\n",
        "# plt.axhline(y=1.96/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray')\n",
        "# plt.title('Autocorrelation Function')\n",
        "# plt.show()\n",
        "# plt.plot(lag_pacf)\n",
        "# plt.axhline(y=0,linestyle='--',color='gray')\n",
        "# plt.axhline(y=-1.96/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray')\n",
        "# plt.axhline(y=1.96/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray')\n",
        "# plt.title('Partial Autocorrelation Function')\n",
        "# plt.show()\n",
        "\n",
        "#implementing the AR Model\n",
        "#here the q value is 0 since its just the AR model\n",
        "model = ARIMA(Train_log, order = (2, 1, 0))\n",
        "# print(model)\n",
        "results_AR = model.fit(disp = -1)\n",
        "# print(result_AR)\n",
        "# print(results_AR.fittedvalues)\n",
        "# print(type(result_AR.fittedvalues))\n",
        "# plt.plot(train_log_diff.dropna(), label = 'original')\n",
        "# plt.plot(results_AR.fittedvalues, color = 'red', label = 'predictions' )\n",
        "# plt.legend(loc = 'best')\n",
        "# plt.show()\n",
        "# we are doing code below to achieve the following points: plot the validation curve for AR model. the curve will a line plot showing the correctness of the prediction on the validation set.\n",
        "# for that We have to change the scale of the model to the original scale.\n",
        "# for that First step would be to store the predicted results as a separate series and observe it.\n",
        "#we are doing this prediction for the validation set range\n",
        "AR_predict = results_AR.predict(start = \"2014-06-25\", end = \"2014-09-25\")\n",
        "# print(AR_predict)\n",
        "AR_predict =  AR_predict.cumsum().shift().fillna(0) #don't know why we did that\n",
        "# print(AR_predict)\n",
        "# print('break')\n",
        "# print(np.log(valid['Count'])[0])\n",
        "AR_predict1 = pd.Series(np.ones(valid.shape[0])* np.log(valid['Count'])[0], index = valid.index) # why are we doing this?\n",
        "# print(AR_predict1)\n",
        "# print('break')\n",
        "AR_predict1 =  AR_predict1.add(AR_predict, fill_value = 0) #why do we do this?\n",
        "# print(AR_predict1)\n",
        "# print('break')\n",
        "AR_predict = np.exp(AR_predict1) #why do we do this?\n",
        "# print(AR_predict)\n",
        "#plot the predictions of the validation set range as a validation curve for AR model\n",
        "# plt.plot(valid[\"Count\"], label = 'valid')\n",
        "# plt.plot(AR_predict, color = 'red', label = 'AR predictions')\n",
        "# plt.legend(loc = 'best')\n",
        "# plt.title('RMSE: %.4f' %(np.sqrt(np.dot(AR_predict, valid['Count']))/valid.shape[0])) # dont know why are we taking the dot product in mse computation\n",
        "# #plt.title('RMSE: %.4f' %(np.sqrt(mean_squared_error(valid.Count, AR_predict))))\n",
        "# plt.show()\n",
        "\n",
        "#implement the MA Model\n",
        "model = ARIMA(Train_log, order = (0, 1, 2))\n",
        "results_MA = model.fit(disp = -1)\n",
        "# plt.plot(train_log_diff.dropna(), label = 'original')\n",
        "# plt.plot(results_MA.fittedvalues, color = 'r', label = 'predicted')\n",
        "# plt.legend(loc = 'best')\n",
        "# plt.show()\n",
        "#plot the MA validation curve, follow the same steps as for AR validation curve\n",
        "MA_predict=results_MA.predict(start=\"2014-06-25\", end=\"2014-09-25\")\n",
        "MA_predict=MA_predict.cumsum().shift().fillna(0)\n",
        "MA_predict1=pd.Series(np.ones(valid.shape[0]) * np.log(valid['Count'])[0], index = valid.index)\n",
        "MA_predict1=MA_predict1.add(MA_predict,fill_value=0)\n",
        "MA_predict = np.exp(MA_predict1)\n",
        "#plot the validation curve now\n",
        "# plt.plot(valid['Count'], label = \"Valid\")\n",
        "# plt.plot(MA_predict, color = 'red', label = \"Predict\")\n",
        "# plt.legend(loc= 'best')\n",
        "# plt.title('RMSE: %.4f'% (np.sqrt(np.dot(MA_predict, valid['Count']))/valid.shape[0]))\n",
        "# plt.show()\n",
        "\n",
        "#combine both the models\n",
        "# print(Train_log)\n",
        "# print(\"break\")\n",
        "model = ARIMA(Train_log, order = (2,1,2))\n",
        "results_ARIMA = model.fit(disp = -1)\n",
        "# plt.plot(train_log_diff.dropna(), label = 'original')\n",
        "# plt.plot(results_ARIMA.fittedvalues, color = 'red', label = 'prediected_ARIMA')\n",
        "# plt.legend(loc = 'best')\n",
        "# plt.show()\n",
        "\n",
        "#plotting the validation curve like we did for both AR and MA models\n",
        "#first lets predict the values for validation set using ARIMA model\n",
        "ARIMA_predict_diff = results_ARIMA.predict(start=\"2014-06-25\", end=\"2014-09-25\")\n",
        "# print(ARIMA_predict_diff)\n",
        "# print('break')\n",
        "# inorder to plot the validation curve for arima model, we need to first create a function which can be used to\n",
        "# change the scale of the model to the original scale.\n",
        "# what names of variables should have been\n",
        "# ARIMA_predict_diff: ARIMA_prediction\n",
        "# given_set: valid_set\n",
        "#predict_diff: prediction\n",
        "def check_prediction_diff(predict_diff, given_set):\n",
        "    predict_diff = predict_diff.cumsum().shift().fillna(0)\n",
        "    print(predict_diff)\n",
        "    predict_base = pd.Series(np.ones(given_set.shape[0]) * np.log(given_set['Count'])[0], index = given_set.index)\n",
        "    print(predict_base)\n",
        "    #log transformation for predicted values\n",
        "    predict_log = predict_base.add(predict_diff, fill_value = 0)\n",
        "    # print(predict_log)\n",
        "    predict = np.exp(predict_log)\n",
        "    # print('break')\n",
        "    # print(predict)\n",
        "\n",
        "    #plot the curve\n",
        "    # plt.plot(given_set['Count'], label = 'valid')\n",
        "    # plt.plot(predict, color = 'red', label = 'prediction')\n",
        "    # plt.plot(loc = 'best')\n",
        "    # plt.title('RSME: %.4f' %(np.sqrt(np.dot(predict, given_set['Count']))/given_set.shape[0]))\n",
        "    # print('RMSE:', np.sqrt(np.dot(predict, given_set['Count']))/given_set.shape[0])\n",
        "    # plt.show()\n",
        "# check_prediction_diff(ARIMA_predict_diff, valid)\n",
        "\n",
        "#scale back the predictions to the original scale\n",
        "def check_prediction_scale(predict, given_set):\n",
        "    predict = predict.cumsum().shift().fillna(0)\n",
        "    # print(predict)\n",
        "    # print(valid.index.append(given_set.index))\n",
        "    predict_base = pd.Series(np.ones(valid.shape[0] + given_set.shape[0]) * np.log(valid['Count'])[0], index = valid.index.append(given_set.index))\n",
        "    # print(predict_base)\n",
        "    #log transformation for predicted values\n",
        "    predict_log = predict_base.add(predict, fill_value = 0)\n",
        "    # print(predict_log)\n",
        "    predict = np.exp(predict_log)\n",
        "    # print('break')\n",
        "    # print(predict)\n",
        "    return predict\n",
        "\n",
        "# print(test)\n",
        "# print(np.log(valid['Count'])[0])\n",
        "#predicting on test data using ARIMA\n",
        "predict = results_ARIMA.predict(start = '2014-06-25', end = '2015-04-26')\n",
        "# print(predict)\n",
        "# check_prediction_scale(predict, test)\n",
        "# print(check_prediction_scale(predict, test))\n",
        "\n",
        "\n",
        "\n",
        "# Note that these are the daily predictions and we need hourly predictions.\n",
        "# So, we will distribute this daily prediction into hourly counts.\n",
        "# To do so, we will take the ratio of hourly distribution of passenger count from train data and\n",
        "# then we will distribute the predictions in the same ratio.\n",
        "test['prediction'] = check_prediction_scale(predict, test)\n",
        "# Remember this is the daily predictions. We have to convert these predictions to hourly basis.\n",
        " # * To do so we will first calculate the ratio of passenger count for each hour of every day.\n",
        " # * Then we will find the average ratio of passenger count for every hour and we will get 24 ratios.\n",
        " # * Then to calculate the hourly predictions we will multiply the daily prediction with the hourly ratio.\n",
        " # every daily prediction is going to be multiplied by 24 such avg ratios of passenger count belonging to 24 hours in a day giving us the prediction of passenger count for each hour in a day.\n",
        "#calculating the hourly ratio of count of passengers\n",
        "# print(train_original)\n",
        "train_original['ratio'] = train_original['Count']/train_original['Count'].sum()\n",
        "# print(train_original)\n",
        "#grouping the hourly ratio\n",
        "temp = train_original.groupby(['Hour'])['ratio'].sum()\n",
        "# print(temp)\n",
        "#groupby to csv format\n",
        "pd.DataFrame(temp, columns = ['Hour', 'ratio']).to_csv('Groupby.csv')\n",
        "temp2 = pd.read_csv(\"Groupby.csv\")\n",
        "# print(temp2)\n",
        "temp2 = temp2.drop(\"Hour.1\",1)\n",
        "# print(temp2)\n",
        "#merge test and test_original on day, month and year\n",
        "# print(test)\n",
        "# print(test_original)\n",
        "# test_original['Day'].astype(float)\n",
        "# test_original['Month'].astype(float)\n",
        "# test_original['Year'].astype(float)\n",
        "# print(test.dtypes)\n",
        "# print(test_original.dtypes)\n",
        "merge = pd.merge(test, test_original, on=('Day', 'Month', 'Year'), how = 'left')\n",
        "merge['Hour'] = merge['Hour_y']\n",
        "merge = merge.drop(['Datetime', 'Hour_y', 'Hour_x', 'Year', 'Month'], axis = 1)\n",
        "# print(merge)\n",
        "#predicting by merging merge and temp2\n",
        "# print(temp2)\n",
        "prediction = pd.merge(merge, temp2, on = 'Hour', how = 'left')\n",
        "# print(prediction)\n",
        "# print(type(prediction))\n",
        "\n",
        "#converting the ratio to the original scale\n",
        "prediction['Count'] = prediction['prediction']*prediction['ratio']*24\n",
        "prediction['ID'] = prediction['ID_y']\n",
        "# print(prediction)\n",
        "\n",
        "#lets drop all the features for the submission file, keep ID and Count only\n",
        "submission=prediction.drop(['ID_x', 'Day', 'ID_y','prediction','Hour', 'ratio'],axis=1)\n",
        "print(\"predictions made on the test data set\\n\")\n",
        "print(submission)\n",
        "\n",
        "# Converting the final submission to csv format\n",
        "# pd.DataFrame(submission, columns=['ID','Count']).to_csv('ARIMA.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#implementing SARIMA\n",
        "# y_hat_avg = valid.copy()\n",
        "# fit1 = sm.tsa.statespace.SARIMAX(Train.Count, order=(2, 1, 4),seasonal_order=(0,1,1,7)).fit()\n",
        "# y_hat_avg['SARIMA'] = fit1.predict(start=\"2014-6-25\", end=\"2014-9-25\", dynamic=True)\n",
        "# plt.figure(figsize=(16,8))\n",
        "# plt.plot( Train['Count'], label='Train')\n",
        "# plt.plot(valid['Count'], label='Valid')\n",
        "# plt.plot(y_hat_avg['SARIMA'], label='SARIMA')\n",
        "# plt.legend(loc='best')\n",
        "# plt.show()\n",
        "# predict=fit1.predict(start=\"2014-9-26\", end=\"2015-4-26\", dynamic=True)\n",
        "# print(predict)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}